[{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected Security Pillar” Event Objectives Understand the Security Pillar in Well-Architected and core principles: Least Privilege, Zero Trust, Defense in Depth Clarify the Shared Responsibility Model and common cloud threats in Vietnam Practice IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response at a foundational level Speakers AWS Solutions Architect Team – Security Specialists AWS Security \u0026amp; Compliance Experts AWS Cloud Operations Team Key Highlights Opening \u0026amp; Security Foundation (8:30 – 8:50) Role of the Security Pillar; core principles: least privilege, zero trust, defense in depth Shared Responsibility Model; top cloud threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management (8:50 – 9:30) IAM: Users, Roles, Policies; avoid long-term credentials IAM Identity Center: SSO, permission sets; SCP and permission boundaries for multi-account MFA, credential rotation, Access Analyzer; mini demo to validate/simulate policy Pillar 2 — Detection (9:30 – 9:55) CloudTrail (org-level), GuardDuty, Security Hub Multi-layer logging: VPC Flow Logs, ALB/S3 logs; alerting and automation with EventBridge Detection-as-Code: infrastructure + control rules Coffee Break (9:55 – 10:10) Pillar 3 — Infrastructure Protection (10:10 – 10:40) VPC segmentation; public vs private placement Security Groups vs NACLs: application models WAF + Shield + Network Firewall; baseline workload protection for EC2/ECS/EKS Pillar 4 — Data Protection (10:40 – 11:10) KMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store: rotation patterns, separated access Data classification and access guardrails Pillar 5 — Incident Response (11:10 – 11:40) AWS IR lifecycle; playbooks: compromised IAM key, S3 public exposure, EC2 malware Snapshot, isolation, evidence collection; auto-response with Lambda/Step Functions Wrap-Up \u0026amp; Q\u0026amp;A (11:40 – 12:00) Recap of 5 pillars; common pitfalls and Vietnam enterprise realities Security learning roadmap: Security Specialty, SA Pro Key Takeaways Design Mindset Design with least privilege, zero trust, defense in depth; enforce org-level guardrails Make shared responsibility explicit; audit and transparency for access Track metrics: time-to-detect, time-to-contain, logging coverage Technical Architecture Solid IAM: SSO/Identity Center, short-lived credentials, mandatory MFA Centralized logging: org-level CloudTrail, VPC Flow Logs, ALB/S3 logs; GuardDuty + Security Hub Network \u0026amp; workload protection: SG/NACL per layer, WAF/Shield/Network Firewall, baselines for ECS/EKS/EC2 Encryption \u0026amp; secrets: clear KMS key policies, rotation, Secrets Manager/Parameter Store Operational Strategy Detection-as-Code and auto-remediation with EventBridge + Lambda/Step Functions Ready IR playbooks: rotate/lock keys, isolate instances, capture evidence Skills/cert path: Security Specialty, SA Pro; tabletop practice Applying to Work Enable org-level CloudTrail, GuardDuty, Security Hub; wire alerts via EventBridge Standardize IAM: enforce MFA, SSO/permission sets, SCP/permission boundaries for multi-account Network design: separate public/private, principle-of-least-privilege SGs, review NACLs Turn on encryption and manage keys with KMS; use Secrets Manager/Parameter Store with rotation Write IR playbooks for leaked IAM key, public S3, suspected EC2 malware; automate isolation and evidence capture Event Experience Learned how local threats map to the 5 pillars from AWS security experts Saw demos on IAM policy simulation and enabling detection/monitoring services Discussed IR playbooks and automating first-response steps "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS Partner Network (APN) Blog Powering European Innovation: Partner Solutions for AWS European Sovereign Cloud by Max Peterson on 02 JUL 2025\nIn: APN Launches, Partner Solutions\nAuthor: Max Peterson, VP AWS Sovereign Cloud As we get closer to launching the AWS European Sovereign Cloud—a new, independent cloud for Europe—we’re excited to see our partner ecosystem continue to grow. The AWS European Sovereign Cloud will be located and operated entirely within the European Union (EU), offering customers the security, availability, and performance of today’s AWS, with added capabilities tailored to evolving EU data-sovereignty requirements.\nToday, we’re pleased to highlight a broadened set of partner solutions being built for AWS European Sovereign Cloud. These complement the strong portfolio announced last November from leading names like Adobe, Cisco, Dedalus, Esri, Genesys, GitLab, Mendix, Pega, Trend Micro, and Wiz. Together they form a comprehensive ecosystem that helps European organizations innovate with confidence.\nThe diversity of these solutions underscores the strong momentum across Europe and our commitment to meet changing data-sovereignty needs. From data analytics and security to industry-specific applications, APN partners are delivering tools and services designed to comply with EU data-sovereignty requirements. These solutions inherit the strict security standards and operational independence of the AWS European Sovereign Cloud, while adding features focused on data sovereignty.\nExpanding Solution Categories and Highlights Data \u0026amp; Analytics Platforms These solutions help organizations unlock data while keeping full control over sovereignty, offering powerful analytics with EU-specific governance. Partners are building platforms that let customers process, store, and analyze data while adhering to EU regulations.\nClickHouse is collaborating to deliver real-time analytics on AWS European Sovereign Cloud, handling billions of records monthly with millisecond latency while meeting strict data-sovereignty needs. It enables advanced analytics and Generative AI while keeping data stored and processed within Europe, complying with GDPR and local rules.\nCloudera is developing a sovereignty-by-design data platform so customers can keep data and metadata entirely within EU borders, ideal for regulated industries and the public sector that must ensure integrity and compliance in Europe.\nMongoDB plans to bring its unified document database platform, simplifying data-driven app development with flexible models, developer-friendly querying, and scalability that aligns with EU data-sovereignty demands.\nReltio is extending its cloud-native data unification platform, Reltio Data Cloud, to the AWS European Sovereign Cloud. The agentic data fabric delivers real-time, trusted, rich-context data, supporting seamless operations, real-time analytics, and intelligent automation—all with EU data-sovereignty compliance. With Reltio Intelligent Data Graph, customers get a comprehensive, millisecond view across customers, products, and suppliers to boost efficiency and experiences.\nSnowflake plans to bring its AI \u0026amp; Data Cloud to AWS European Sovereign Cloud, helping enterprises meet evolving European data-sovereignty and regulatory needs, with comprehensive data governance and security for sensitive data and metadata in a strictly compliant environment.\nObservability \u0026amp; Process Management Solutions here provide deep visibility into applications, infrastructure, and business processes while keeping all customer data and metadata within the EU. Partners combine robust monitoring with process optimization and data-residency controls.\nARIS is building a full Process Intelligence suite to identify, analyze, simulate, optimize, and control business processes within the EU, including process mining and modeling, cost control, process governance, and compliance.\nd.velop plans to deploy its enterprise-grade Document Management System (DMS) to manage and store documents with compliant data protection, optimize workflows, and ensure adherence to strict legal standards.\nSumo Logic is creating a unified, AI-powered cloud-native log analytics platform, giving DevSecOps teams a single data source for reliability, security, and cloud insights—while keeping data sovereign in the EU.\nIntegration, DevOps \u0026amp; Migration These tools support secure, compliant software development, deployment, integration, and migration while preserving operational sovereignty in the EU—covering the full modernization lifecycle with data-sovereignty at the center.\nBoomi is developing a dedicated version of Boomi Enterprise Platform, an AI-driven integration and automation platform connecting apps, APIs, data, and AI agents, reinforcing metadata security and EU data sovereignty.\nEficode is building Eficode ROOT for AWS European Sovereign Cloud, combining a managed development platform with DevOps expertise to run secure, scalable, compliant environments.\nTrianz is creating Concierto, a highly automated, no-code SaaS platform to accelerate end-to-end cloud migration and modernization with intelligent automation, hybrid-cloud operations, FinOps, optimization, and auto-remediation via a single pane of glass—reducing technical debt while maintaining control and sovereignty.\nAI \u0026amp; Machine Learning AI/ML solutions process and analyze data within the EU, enabling innovation while maintaining data sovereignty.\nMistral AI plans to offer its models and applications, including Le Chat Enterprise, on AWS European Sovereign Cloud—providing secure enterprise search and custom AI agents to automate tasks across multiple data sources, with compliance for high-security sectors such as defense, public sector, and financial services.\nSecurity \u0026amp; Identity Management Security and identity solutions protect sensitive workloads while maintaining strict data control, tailored for the European cloud environment.\nCyberArk plans to bring its AI-powered Identity Security Platform, applying intelligent privileged controls to human, machine, and AI identities—reducing operational and security risks via Zero Trust and Least Privilege with full visibility in a sovereign environment.\nThales is developing custom cloud-security offerings: CipherTrust Cloud Key Manager for key control, CipherTrust Data Security Platform for end-to-end data protection, Thales CIAM for secure user authentication, and Imperva WAAP to maintain data sovereignty at the application layer—helping customers protect sensitive data and comply while using cloud services safely.\nWithSecure is creating a comprehensive, proactive cloud-native security solution with enhanced visibility and protection for European workloads, meeting stringent data-sovereignty requirements.\nPartners are tailoring solutions to core digital-sovereignty needs—data residency, restricted operational access, resiliency, and survivability—so customers can build comprehensive, compliant environments suited to their unique requirements. As launch approaches, we’ll keep expanding partner collaborations to deliver more choice and capability. The priority region in Brandenburg, Germany—backed by a €7.8B investment—shows our commitment to European digital sovereignty and partner success.\nCustomers can start planning their move to AWS European Sovereign Cloud today by working with our partners and solution architects to design the right sovereign cloud architecture.\nTogether with partners, we’re building a future where European organizations can innovate confidently, knowing every digital-sovereignty requirement is met without compromise.\nTAGS: ARIS, AWS European Sovereign Cloud, Boomi, ClickHouse, Cloudera, CyberArk, d.velop, Eficode, Mistral AI, MongoDB, Reltio, Snowflake, Sumo Logic, Thales, Trianz, WithSecure\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Data-driven approach: Optimizing on-demand cost of AWS Elemental MediaLive By Krutarth Doshi, May 8, 2025 | in Amazon QuickSight, Analytics, AWS Elemental MediaLive, Data Science \u0026amp; Analytics for Media, Industries, Media \u0026amp; Entertainment, Media Services, Monetization | Permalink\nIn the fast-moving world of live video, optimizing cost while keeping quality high is a constant challenge for media companies. Amazon Web Services (AWS) Elemental MediaLive, a cloud live video processing service, offers flexible pricing options to help. Finding the right balance between on-demand and reserved requires careful analysis of usage patterns.\nThis post walks through a data-driven approach to managing MediaLive reservations. By leveraging AWS Cost and Usage Reports (AWS CUR) and Amazon QuickSight, you can gain insights into MediaLive usage to make informed reservation decisions and reduce cost.\nBefore the analysis, recap MediaLive pricing options:\nOn-Demand pricing: Pay only for resources you use; you pay hourly for inputs, outputs, and add-on features. Maximum flexibility but potentially higher cost for steady usage. Reserved pricing: Commit to MediaLive for 12 months for discounts up to 70 percent vs on-demand. Significant savings for predictable workloads. The key is finding the right on-demand vs reserved mix based on your usage patterns.\nPrerequisites You need the following before trying the solution:\nProper access to AWS CUR, Amazon Athena, and QuickSight. Follow least-privilege IAM (see IAM best practices) and do your own due diligence.\nEnable AWS CUR if not already. See Creating Cost and Usage Reports.\nCUR granularity must be hourly for this solution so usage patterns are accurate for reservation decisions. Set up Athena to analyze data in the S3 bucket that stores your CUR. See Querying Cost and Usage Reports using Amazon Athena.\nSet up QuickSight to visualize data from your Athena view. See Setting up for Amazon QuickSight.\nSolution Create an Amazon Athena database for AWS CUR data If this is your first time using CUR with Athena, create an Athena database and table on CUR data. Choose one of:\nUse AWS CloudFormation (recommended) to integrate CUR with Athena. Or\nCreate the Athena table manually. You must repeat monthly because the table only covers the current CUR path. Create an Amazon Athena view from AWS CUR to query insights An Athena view is a logical table that stores a SQL query as a virtual table, always reading the latest data.\nTo create the view eml_od_usage, copy the SQL from the CUR Query Library (MediaLive OnDemand Usage) into the Athena editor.\nReplace the ${table_name} placeholder before running. For example, if your CUR table is cur_table in database cur_db, replace ${table_name} with cur_db.cur_table.\nPrefix the query with:\nCREATE OR REPLACE VIEW \u0026#34;eml_od_usage\u0026#34; AS then run it.\nFigure 1: Create an Amazon Athena view.\nThis view feeds QuickSight dashboards to visualize MediaLive spend over time.\nConfigure Amazon QuickSight to access the Athena view Next, create a QuickSight dataset using the Athena views.\nAdd the view eml_od_usage from Athena views into QuickSight as a dataset.\nFigure 2: Add an Amazon Athena view to a QuickSight dataset.\nChoose query mode SPICE. Click PUBLISH \u0026amp; VISUALIZE to create a new QuickSight analysis.\nFigure 3: Preview a QuickSight dataset before visualizing\nAfter adding the dataset, add a refresh schedule so it receives new CUR data. Daily or weekly refresh is recommended for better visuals.\nFigure 4: Add calculated fields to a QuickSight dataset\nBuild visuals for MediaLive reservations After adding the dataset, add visuals in the analysis. QuickSight analyses let users create interactive visuals and explore data with filtering, calculations, and custom views.\nFollow these steps:\nFrom the analysis page, add a visual On-Demand spend by Account ID.\nLeft sidebar: click Add in Visual types. Choose Pie chart. In Data, select eml_od_usage (Figure 5). Add fields line_item_usage_account_id and sum_line_item_unblended_cost. Change sum_line_item_unblended_cost aggregation to Sum.\nFigure 5: QuickSight pie/table showing On-Demand cost by Account ID and Input Usage Type.\nAdd On-Demand cost by Account ID and Usage Type.\nClick Add; choose Table. Ensure eml_od_usage is selected. Add fields line_item_usage_account_id, line_item_usage_type, round_sum_line_item_usage_amount, sum_line_item_unblended_cost. Set round_sum_line_item_usage_amount and sum_line_item_unblended_cost to Sum.\nFigure 6: QuickSight table of recommended reservations by account and usage type.\nAdd Recommended reservations by Usage Type.\nClick Add; choose Table. Select eml_od_usage. Add line_item_usage_type, round_sum_line_item_usage_amount, sum_line_item_unblended_cost, Recommended reservation count. Set the two numeric cost fields to Sum. Set Recommended reservation count to Average.\nFigure 7: QuickSight chart showing distribution of On-Demand spend.\nAdd Recommended reservations and On-Demand spend over time.\nClick Add; choose Line chart. Select eml_od_usage. Add fields line_item_usage_end_date, Recommended reservation count, sum_line_item_unblended_cost. Aggregate line_item_usage_end_date by WEEK. Set Recommended reservation count to Average; set sum_line_item_unblended_cost to Sum.\nFigure 8: QuickSight line/table for Recommended reservations vs On-Demand Spend.\nAfter these steps\nYou have an analysis page like Figure 9, summarizing MediaLive On-Demand spend and showing reservation recommendations by account and usage type. You can publish as a dashboard and share it to track MediaLive spend and usage over time. Figure 9: QuickSight dashboard for MediaLive cost and reservation recommendations\nMake reservation decisions with data Identify accounts with highest On-Demand cost, then review usage patterns. Combine recommended reservation counts with spend trends to choose reservation size. When building a business case, consider components with steady On-Demand spend and growth potential. Hourly CUR data shows regularity of usage to time purchases for savings. For uneven loads or short campaigns, keep flexibility to avoid overcommitment. Deploy and monitor your reservation strategy After finalizing the strategy, purchase reservations via the AWS Management Console or API. To stay optimal, build monitoring with CUR data and QuickSight dashboards to track reservation utilization. Set proactive alerts for underused reservations or spend spikes. Usage patterns change over time; review and adjust regularly to keep MediaLive delivering maximum value and cost efficiency. Conclusion Using AWS CUR plus Amazon QuickSight visualization, you can optimize AWS Elemental MediaLive cost with strategic reservations. This data-driven approach supports informed decisions balancing savings and flexibility for your media workloads.\nThe key to success is continuous analysis and adjustment. Review CUR data, QuickSight dashboards, and reservation strategy regularly to keep getting maximum value from MediaLive.\nLearn more on the AWS Elemental MediaLive pricing page. Contact an AWS representative for help accelerating your media business.\nKrutarth Doshi\nKrutarth Doshi is a Senior Technical Account Manager at AWS with 10+ years of experience. He has supported ISV customers for the past two years, building custom CUR queries and QuickSight visualizations to solve complex technical challenges.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Automating audio editing and transcoding using AWS by Todd Leishman | on 29 MAY 2025 | in Amazon EC2, Amazon Elastic Transcoder, Amazon Simple Notification Service (SNS), Amazon Simple Queue Service (SQS), Application Services, AWS Batch, AWS Elemental MediaConvert, AWS Fargate, AWS Lambda, Compute, Content Production, Industries, Media \u0026amp; Entertainment, Media Services, Messaging Permalink\nAudio editing and transcoding are crucial processes for content creators, broadcasters, and media professionals who need to convert audio files from one format to another and automate basic audio editing tasks. With Amazon Elastic Transcoder scheduled for deprecation, in November, 2025, it is important to understand the alternatives for implementing audio-only workflows in Amazon Web Services (AWS).\nAudio processing workflows present unique challenges, as most available tools are designed specifically for video, with audio often being an afterthought.\nA common audio-only workflow can involve selecting a short segment from a full music file (Figure 1), clipping that segment (Figure 2), and applying fade-in and fade-out effects to create smooth transitions (Figure 3).\nFigure 1: Selecting a short segment.\nFigure 2: The clipped audio segment.\nFigure 3: Addition of fading in and out audio segments.\nThis workflow can be automated to save time and effort, allowing audio engineers to focus on complex mixing and editing projects.\nAudio processing using FFmpeg, Python, Amazon EC2, and Amazon S3 To get started, you will leverage Amazon Elastic Compute Cloud (Amazon EC2), Amazon Simple Storage Service (Amazon S3) and the open-source tools FFmpeg and Python. This solution will lay the groundwork for streamlined, more efficient methods using AWS Lambda and AWS Elemental MediaConvert.\nS3 bucket configuration Create a new S3 bucket* with two prefixes (folders), one for original files and one for converted files. For example:\nhttps://s3.us-west-2.amazonaws.com/audiobucket/original/ https://s3.us-west-2.amazonaws.com/audiobucket/converted/ *Ensure that your bucket name is unique\nSetup IAM roles and policies for accessing the new S3 bucket\nUpload your original .flac or .wav audio files to the original files prefix (folder) using the AWS Management Console (the console)\nEC2 instance configuration Launch a Linux t3a.medium EC2 instance or similar (for our following example solution we will be using Amazon Linux 2) Connect to the new instance by using Secure Shell (SSH) or the console Install FFmpeg on the EC2 instance by running the following commands in the terminal: $ sudo su # cd /usr/local/bin # mkdir ffmpeg # cd ffmpeg # wget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz # tar -xf ffmpeg-release-amd64-static.tar.xz # cd ffmpeg-7.0.2-amd64-static/ # cp -a ffmpeg /usr/bin/ # cp -a ffprobe /usr/bin/ # cp -a qt-faststart /usr/bin/ # cd /usr/bin # ffmpeg -version The ffmpeg -version command should return a result similar to this: ffmpeg version 7.0.2-static https://johnvansickle.com/ffmpeg/ ...\nInstall the AWS Command Line Interface (AWS CLI) on your EC2 instance (if not using Amazon Linux) How to install AWS CLI on Linux Install Python on your EC2 instance (if not using Amazon Linux) Install Python on Rocky Linux Install Mountpoint for Amazon S3 on your EC2 instance (or use the S3 Sync API if preferred) Installing Mountpoint video: How To Mount An Amazon S3 Bucket As A Local Drive Step-by-Step guide Mountpoint credentials configuration options Mount your S3 bucket using S3 Mountpoint # mount-s3 audiobucket /mnt/audiobucket Make sure you can see the files you uploaded to the S3 bucket /audiobucket/original/ directly on the EC2 instance through the terminal: # cd /mnt/audiobucket/original # ls -lah Environment Testing Now that your AWS environment is prepared, go ahead and test the solution by converting an original FLAC file to an AAC file, while simultaneously clipping and fading the audio in and out.\nTest the conversion of a single file located in S3 using a BASH FFmpeg command and Mountpoint for Amazon S3. ffmpeg -i /mnt/audiobucket/original/audiofile-01.flac -ss 00:00:03.000 -to 00:00:15.000 -vn -c:a libfdk_aac -af afade=in:st=3:d=2,afade=out:st=10:d=4 /mnt/audiobucket/converted/audiofile-01.aac This FFmpeg command assumes the file is longer than 15 seconds and it includes the following options:\n-i = input file (audiofile-01.flac) -ss = clip start time (3 seconds) -to = end clip time (15 seconds) -vn = video no (audio-only file) -c:a = selects audio encoder (use the libfdk aac encoder) -af = audio filter (add a filter to the audio file (fade in this case)) afade=in = apply the fade in (fade audio in) st = fade in start time (start the fade in at 3 seconds) d = fade in duration (fade the audio in for 2 seconds) afade=out = apply the fade out (fade audio out) st = fade out start time (start the fade out at 10 seconds) d = fade out duration (fade the audio out for 4 seconds) Your newly converted file will be saved to your S3 buckets /converted/ prefix (folder). Open the file to verify that fading, clipping and transcoding occurred.\nPython scripting for more control For more advanced automation, you can use Python with the subprocess module to call FFmpeg, which will complete the edit and conversion. This will give us more control of our audio workflow, including adding a variety of audio formats. You can also check to see if a file has already been converted before proceeding with a new conversion workflow.\nThere are several ways to create the script. Following you will find a basic example, which can be altered for each use case. This Python script is based on the BASH command you tested previously, and will convert .wav or FLAC files to AAC format files, while trimming and fading the files. This script will convert all files located in the original folder used previously, unless a converted version already exists in the converted folder. For production use, it is recommended to enhance the Python script by adding input validation, error handling for unexpected files, and additional logging for success, failure or errors.\nCreate the new python script on the EC2 instance using Vim or your preferred editor. In this case, you will name the script convert_audio.py.\n$ sudo vi convert_audio.py Paste the following script into the new document:\nimport subprocess import os import glob import sys def convert_audio_files(original_dir, converted_dir): if not os.path.exists(converted_dir): os.makedirs(converted_dir) supported_formats = (\u0026#39;\u0026#39;.flac\u0026#39;\u0026#39;, \u0026#39;\u0026#39;.wav\u0026#39;\u0026#39;) for file_extension in supported_formats: for input_file in glob.glob(os.path.join(original_dir, \u0026#39;\u0026#39;*\u0026#39;\u0026#39; + file_extension)): file_name = os.path.basename(input_file) output_file = os.path.join(converted_dir, os.path.splitext(file_name)[0] + \u0026#39;\u0026#39;.aac\u0026#39;\u0026#39;) if os.path.exists(output_file): print(f\u0026#34;Skipping {file_name} as it already exists in the converted directory.\u0026#34;) continue command = [ \u0026#34;ffmpeg\u0026#34;, \u0026#34;-i\u0026#34;, input_file, \u0026#34;-ss\u0026#34;, \u0026#34;00:00:03.000\u0026#34;, \u0026#34;-to\u0026#34;, \u0026#34;00:00:15.000\u0026#34;, \u0026#34;-vn\u0026#34;, \u0026#34;-c:a\u0026#34;, \u0026#34;libfdk_aac\u0026#34;, \u0026#34;-af\u0026#34;, \u0026#34;afade=in:st=3:d=2,afade=out:st=10:d=4\u0026#34;, output_file ] subprocess.run(command) if __name__ == \u0026#39;\u0026#39;__main__\u0026#39;\u0026#39;: if len(sys.argv) != 3: print(\u0026#34;Usage: python script.py \u0026lt;original_directory\u0026gt; \u0026lt;converted_directory\u0026gt;\u0026#34;) sys.exit(1) original_directory = sys.argv[1] converted_directory = sys.argv[2] convert_audio_files(original_directory, converted_directory) Save and close your editor.\nInitiate the script, convert_audio.py, by including the input and output folders within the command:\n$ sudo python3 convert_audio.py /mnt/audiobucket/original /mnt/audiobucket/converted Simple automation setup Now that you have tested the conversion script manually, you can setup your EC2 instance to automatically run the script every 10 minutes. As new files are added to the original folder, this simple automation will pick up those new files and run the conversion every 10 minutes. The python script verifies that only new files are converted.\nTo make the cron job run every 10 minutes daily, set the minute field to a step value of 10. Heres how to modify the cron job:\nGo to the EC2 instance terminal Type crontab -e and press Enter to open the crontab file Add the following line to the crontab file, replacing /path/to/convert_audio.py with the actual path to your Python script (the cron job fields represent minutes, hours, day of the month, month, and day of the week, respectively): */10 * * * * /usr/bin/python3 /path/to/convert_audio.py /mnt/audiobucket/original /mnt/audiobucket/converted Save the cron job changes and exit the editorthe cron job will now run your Python script every 10 minutes, every day. Summary Audio processing workflows present unique challenges, as most available tools are designed specifically for video, with audio often being an afterthought. With Amazon Elastic Transcoder scheduled for deprecation, it is important to understand the alternatives for implementing audio-only workflows using AWS services.\nThis blog is the first in a three-part series that explored basic automated audio conversion using AWS services and open-source tools.\nIn the next blog, we will streamline the workflow by utilizing AWS Lambda to create a serverless audio conversion service.\nContact an AWS Representative to know how we can help accelerate your business.\nFurther reading Transcoding with FFmpeg on AWS Graviton Processors Installing FFmpeg on AWS Graviton Instances Todd Leishman Todd Leishman has worked in the Media \u0026amp; Entertainment industry for over 25 years, focusing on audio engineering, live event streaming, and media asset management. As an audiophile, Todd is always looking for new ways to improve audio technologies and techniques using cloud-based solutions.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “AWS AI/ML \u0026amp; Bedrock Workshop” Event Objectives Network and clarify learning goals; understand the AI/ML landscape in Vietnam Get an overview of AWS AI/ML services with focus on SageMaker (end-to-end ML) and Bedrock (GenAI) Align objectives from data prep, training/deploy to building a GenAI chatbot Speakers Kha Van Kiet Lam Aiden Dinh Key Highlights Welcome \u0026amp; Introduction (8:30 – 9:00) Registration, networking, sharing PoC expectations Ice-breaker to surface use cases: internal knowledge chatbot, document classification, CS automation Snapshot of local AI/ML landscape: data security needs, GenAI opportunities AWS AI/ML Services Overview (9:00 – 10:30) Amazon SageMaker: data prep, labeling, training, tuning, deployment Integrated MLOps: pipelines, experiment tracking, model registry, canary/blue-green for endpoints Cost optimization: spot training, endpoint autoscaling, right-size instances Demo SageMaker Studio: unified notebook + tracking + deploy Coffee Break (10:30 – 10:45) Quick discussions on internal use cases and data constraints Generative AI with Amazon Bedrock (10:45 – 12:00) Foundation Models: Claude, Llama, Titan — selection by use case (reasoning, extensibility, AWS integration) Prompt engineering: chain-of-thought, few-shot, control length and confidence RAG: vector store + knowledge base, reduce hallucination, fast data refresh Bedrock Agents: orchestrate multi-step workflows, integrate external tools/systems Guardrails: policies to filter sensitive content, PII controls Demo: build a QA chatbot on enterprise data using Bedrock + RAG Key Takeaways Design Mindset Start from business problems and internal data before picking models Design for safety: guardrails, sensitive-topic filtering, PII controls Define metrics: answer quality, time-to-deploy, inference cost Technical Architecture MLOps with SageMaker: pipelines, model registry, canary/blue-green endpoints RAG for GenAI: choose vector store/knowledge base, data refresh strategy, access controls Pragmatic prompt engineering: few-shot, chain-of-thought, control length and confidence Modernization Strategy Select Foundation Model per use case (reasoning, extensibility, AWS fit) Optimize costs early: spot training, endpoint autoscaling, monitor drift to avoid waste Roadmap PoC → pilot → production with safety/compliance checklist Applying to Work Prepare an internal QA chatbot PoC using Bedrock + RAG on enterprise docs Standardize train/tune/deploy with SageMaker; use model registry and canary/blue-green rollout Create a prompt-engineering and guardrails checklist for product teams Estimate and optimize costs: spot training, autoscaling, monitor drift and quality Event Experience Attending the AWS AI/ML \u0026amp; Bedrock workshop clarified how to apply GenAI and MLOps to real problems.\nLearning from speakers Criteria to choose Foundation Models (Claude, Llama, Titan) and operate SageMaker at scale How to enforce safety/content controls with guardrails and input/output policies Technical exposure Demo of SageMaker Studio: notebook + tracking + deploy in one interface Demo of Bedrock + RAG chatbot on enterprise data, understanding the ingest → index → query flow Practical application Clear path to build an internal knowledge chatbot with Bedrock Agents and RAG How to set autoscaling/cost controls and canary/blue-green for SageMaker endpoints Lessons learned Begin with business/data and guardrails before scaling models Standardize ML/GenAI lifecycle with pipelines, registry, drift and quality monitoring "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Objectives Shape DevOps mindset, culture, and metrics (DORA, MTTR, deployment frequency) Learn AWS DevOps services: CodeCommit, CodeBuild, CodeDeploy, CodePipeline Practice IaC with CloudFormation/CDK and compare IaC tool choices Bridge prior AI/ML learnings into CI/CD, observability, and operations Speakers -AWS Solutions Architect Team – DevOps Specialists -AWS Container Services Experts -AWS Observability and Cloud Operations Team\nKey Highlights Welcome \u0026amp; DevOps Mindset (8:30 – 9:00) Quick recap of prior AI/ML session; align DevOps goals DevOps culture \u0026amp; principles; benefits and key metrics (DORA, MTTR, deployment frequency) AWS DevOps Services – CI/CD Pipeline (9:00 – 10:30) Source control: CodeCommit; GitFlow vs Trunk-based strategies Build \u0026amp; test: CodeBuild pipeline configuration Deploy: CodeDeploy with blue/green, canary, rolling Orchestration: CodePipeline end-to-end automation Demo: full CI/CD pipeline walkthrough Break (10:30 – 10:45) Infrastructure as Code (10:45 – 12:00) CloudFormation: templates, stacks, drift detection AWS CDK: constructs, reusable patterns, language support Demo: deploy with CloudFormation \u0026amp; CDK; discuss IaC tool selection Lunch (12:00 – 13:00) – self-arranged Container Services on AWS (13:00 – 14:30) Docker fundamentals, microservices \u0026amp; containerization ECR: image storage, scanning, lifecycle policies ECS \u0026amp; EKS: deployment strategies, scaling, orchestration App Runner: simplified container deployment Demo \u0026amp; case study: microservices deployment comparison Break (14:30 – 14:45) Monitoring \u0026amp; Observability (14:45 – 16:00) CloudWatch: metrics, logs, alarms, dashboards X-Ray: distributed tracing and performance insights Demo: full-stack observability setup; alerting/on-call best practices DevOps Best Practices \u0026amp; Case Studies (16:00 – 16:45) Deployment strategies: feature flags, A/B testing Automated testing \u0026amp; CI/CD integration Incident management and postmortems; startup/enterprise case studies Q\u0026amp;A \u0026amp; Wrap-up (16:45 – 17:00) DevOps career pathways; AWS certification roadmap Key Takeaways Design Mindset DevOps is culture and process: prioritize flow, feedback, continuous learning Measure with DORA/MTTR/deploy frequency; set guardrails to enable, not block Connect AI/ML pipelines into CI/CD for fast but safe releases Technical Architecture Design CI/CD with CodeCommit/CodeBuild/CodeDeploy/CodePipeline Choose deployment strategies: blue/green, canary, rolling; add feature flags/A-B testing IaC: CloudFormation vs CDK, drift detection, reusable constructs Full-stack observability: CloudWatch metrics/logs/alarms + X-Ray tracing Modernization Strategy Container-first: ECR + ECS/EKS; App Runner for quick service deployments Standardize security/compliance in pipelines (image scanning, policy, least privilege) Skills and certification path for AWS DevOps; plan PoC → pilot → production Applying to Work Set up a sample CI/CD repo with CodeCommit + CodeBuild + CodeDeploy + CodePipeline Apply blue/green or canary to existing services; add feature flags to reduce risk Write IaC with CDK for a small workload; compare with CloudFormation templates Standardize observability: CloudWatch dashboards/alarms, X-Ray for key traces Manage images in ECR: enable scan, lifecycle policies to clean old images Event Experience Attending “DevOps on AWS” connected prior AI/ML learnings with CI/CD, IaC, containers, and observability.\nLearning from speakers Git strategies, CI/CD pipelines, and safe deployments (blue/green, canary) CloudFormation vs CDK, drift detection, reusable constructs Technical exposure End-to-end pipeline demo with CodePipeline + CodeBuild + CodeDeploy IaC deployment demo with CloudFormation and CDK Observability demo: CloudWatch metrics/logs/alarms + X-Ray tracing Practical application Clear approach to standardize pipelines and IaC for current projects How to enable scan/lifecycle for ECR and deploy/scale on ECS/EKS/App Runner Lessons learned Measure DevOps with DORA/MTTR and design safe pipelines from day one IaC and observability are foundations to scale teams and reduce deployment risk "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Xuan Truong\nPhone Number: 0389334326\nEmail: truongtxse180558@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.1-introduction/","title":"Introduction","tags":[],"description":"","content":"Architecture Amazon ECR (Elastic Container Registry) Amazon ECR is a fully managed Docker image registry that allows you to securely store, manage, and version your container images. It integrates seamlessly with ECS, CodeBuild, and CodePipeline.\nAWS CodePipeline AWS CodePipeline is a fully managed CI/CD service that automates the software release process. Whenever changes are pushed to the source repository, CodePipeline triggers stages such as Source → Build → Deploy without manual intervention.\nAWS CodeBuild AWS CodeBuild is a fully managed build service that compiles source code, runs tests, builds Docker images, tags them, and pushes them to ECR. All build steps are defined in a simple buildspec.yml file.\nWorkshop Overview In this workshop, you will learn how to build an automated CI/CD pipeline for containerized applications using AWS CodePipeline, CodeBuild, and Amazon ECR. You will create a complete workflow that retrieves source code, builds a Docker image, and pushes it securely to an ECR repository. Throughout the workshop, you will configure essential AWS services, work with buildspec files, and understand how each component contributes to a smooth and automated deployment process.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Goals: Connect and get acquainted with members of the First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit 08/09/2025 08/09/2025 3 - Learn about AWS and its service types + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/ 5 - Learn EC2 basics: + Instance types + AMI + EBS - Ways to SSH into EC2 - Learn about Elastic IP 11/09/2025 11/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ 6 - Practice: + Create EC2 instance + Connect via SSH + Attach EBS volume 12/09/2025 12/09/2025 https://000003.awsstudygroup.com/vi/5-vpnsitetosite/5.1-createvpnenv/5.1.2-createec2vpn/ Week 1 Achievements: Understand what AWS is and grasp the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nGot familiar with AWS Management Console and learned how to find, access, and use services from the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information List regions View EC2 services Create and manage key pairs Check running service information Able to connect and manage AWS resources in parallel via both web interface and CLI.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/","title":"Worklog","tags":[],"description":"","content":"FCJ 3-Month Worklog:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Mastering and deploying basic AWS services\nWeek 3: Learning about VPC Peering, AWS Transit Gateway, and Deploying Wordpress on AWS Cloud\nWeek 4: Continuing Wordpress deployment on AWS Cloud, Getting familiar with Amazon RDS\nWeek 5: Getting familiar with S3 bucket and creating EC2 Instance using AMI Storage Gateway, Deploying FCJ Management application with Auto Scaling Group, and Practicing with Amazon CloudWatch\nWeek 6: Using AWS IAM Identity Center for identity management, Getting familiar with AWS Web Application Firewall, and Managing resources with Tags and Resource Groups\nWeek 7: Managing access to EC2 Resource Tag service with AWS IAM, Getting started with Grafana on AWS, and Limiting user permissions with IAM Permission Boundary\nWeek 8: Preparing and setting up resources for the workshop, Optimizing EC2 costs with Lambda\nWeek 9: Granting AWS service access to applications with IAM Role, Understanding database systems and AWS Database services, Deploying Amazon RDS, Building Data Lake on AWS, and Amazon DynamoDB\nWeek 10: Working with AWS Glue \u0026amp; Amazon Athena, Amazon DynamoDB, Analytics on AWS Workshop, Migrating Monolith application to AWS environment, and Serverless with Lambda interacting with S3 and DynamoDB\nWeek 11: Writing Frontend to call API Gateway, Deploying applications with AWS SAM, Authentication with Amazon Cognito, Setting up static website with SSL on S3, and Handling orders with SQS-SNS\nWeek 12: Serverless CI/CD with CodePipeline and Completing Workshop\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.2-preparation/","title":"Preparation","tags":[],"description":"","content":"IAM Permission Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-northeast-1:833390817075:log-group:/aws/codebuild/CodeBuild-VideoShare\u0026#34;, \u0026#34;arn:aws:logs:ap-northeast-1:833390817075:log-group:/aws/codebuild/CodeBuild-VideoShare:*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::codepipeline-ap-northeast-1-*\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codebuild:CreateReportGroup\u0026#34;, \u0026#34;codebuild:CreateReport\u0026#34;, \u0026#34;codebuild:UpdateReport\u0026#34;, \u0026#34;codebuild:BatchPutTestCases\u0026#34;, \u0026#34;codebuild:BatchPutCodeCoverages\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:codebuild:ap-northeast-1:833390817075:report-group/CodeBuild-VideoShare-*\u0026#34; ] } ] } "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/2-proposal/","title":"Proposal ","tags":[],"description":"","content":"Video Sharing Platform 1.Executive Summary This proposal outlines the development of a scalable Video Sharing Platform leveraging AWS cloud services. The platform will enable users to upload, stream, and share video content with features including user authentication, content management, and real-time video streaming.\nKey objectives:\nBuild a secure, scalable video sharing platform Implement user authentication and authorization Provide high-quality video streaming capabilities Ensure cost-effective infrastructure management Deliver seamless user experience across devices The solution utilizes AWS services including Amplify for frontend hosting, Cognito for authentication, S3 for storage, CloudFront for content delivery, and Interactive Video Service for streaming capabilities.\n2.Problem Statement What\u0026rsquo;s the Problem? Current video sharing solutions face several challenges:\nHigh infrastructure costs for video storage and streaming Complex setup and maintenance requirements Limited scalability during peak usage Security vulnerabilities in user authentication Poor video quality and buffering issues The Solution Our AWS-based video sharing platform addresses these challenges by:\nLeveraging AWS\u0026rsquo;s cost-effective, pay-as-you-use pricing model Utilizing managed services to reduce operational overhead Implementing auto-scaling capabilities for handling traffic spikes Providing enterprise-grade security through AWS Cognito Delivering high-quality video streaming via Amazon IVS and CloudFront Benefits and Return on Investment Cost Savings:\n40-60% reduction in infrastructure costs compared to traditional hosting No upfront hardware investments required Pay-per-use model optimizes operational expenses Performance Improvements:\n99.9% uptime availability Global content delivery with low latency Auto-scaling handles 10x traffic increases seamlessly Business Value:\nFaster time-to-market (3-6 months vs 12+ months) Enhanced user experience drives higher engagement Scalable architecture supports business growth 3.Solution Architecture AWS Services Used Amplify: Frontend hosting and deployment platform for React/Vue.js applications with CI/CD integration.\nCognito: User authentication and authorization service providing secure sign-up, sign-in, and access control.\nApp Runner: Containerized backend API hosting with automatic scaling and load balancing.\nDynamoDB: NoSQL database for storing user profiles, video metadata, and application data.\nS3: Object storage for video files, thumbnails, and static assets with versioning and lifecycle policies.\nCloudFront: Global CDN for fast content delivery and video streaming with edge caching.\nAmazon IVS (Interactive Video Service): Real-time video streaming service for live broadcasts and on-demand content with low latency.\nAWS Elemental MediaConvert: Used to turn original video into many formats, many resolutions, many codecs, for streaming or downloading.\nAWS Lambda: Serverless compute service for event-driven processing, video transcoding triggers, and backend automation tasks.\nCode Pipeline: CI/CD pipeline for automated testing, building, and deployment.\nCode Build: Build service for compiling source code, running tests, and creating deployment packages.\nElastic Container Registry: Docker container registry for storing and managing application images.\nComponent Design Frontend Layer:\nReact-based web application hosted on Amplify Responsive design supporting mobile and desktop Real-time video player with adaptive bitrate streaming API Layer:\nRESTful APIs built with .NET Containerized and deployed on App Runner AWS Lambda for serverless API endpoints JWT-based authentication integration Data Layer:\nDynamoDB tables for user data and video metadata S3 buckets for video storage with intelligent tiering Security Layer:\nCognito user pools for authentication IAM roles and policies for access control Video Processing Layer:\nAWS Lambda functions triggered by S3 events AWS Elemental MediaConvert for transcoding videos Automated conversion to multiple formats and resolutions HLS and DASH output for adaptive streaming Streaming Architecture:\nAmazon IVS for live streaming capabilities CloudFront for global video distribution Adaptive bitrate streaming for optimal quality Use Cases Live Streaming Events:\nReal-time broadcasting of conferences, webinars, and corporate events Multi-bitrate streaming for optimal viewer experience Video On Demand (VOD):\nUpload and share educational content, tutorials, and training materials Secure content access with user permissions Social Video Sharing:\nUser-generated content sharing Community features with comments and ratings 4.Technical Implementation Phase 1: Infrastructure Setup AWS Account Configuration:\nConfigure IAM roles and policies for least privilege access Core Services Deployment:\nDeploy DynamoDB tables with proper indexing Configure S3 buckets with encryption and lifecycle policies Set up Cognito user pools and identity pools Phase 2: Backend Development API Development:\nBuild RESTful APIs using Node.js/Express framework Implement JWT authentication with Cognito integration Create video upload/processing endpoints Develop Lambda functions for event-driven tasks Develop user management and content APIs Database Schema:\nUsers table: userId, avatarUrl, birthDate, channelId, createdAt, email, gender, name, phoneNumber Videos table: videoId, channelId, commentCount, createdAt, createdFromStreamId, description, duration, key, likeCount, playbackUrl, status, thumbnailUrl, title, type, updatedAt, userId, viewCount VideoLikes table: userId, videoId, createdAt Subscriptions table: userId, channelId, createdAt StreamSessions table: streamId, channelId, createdAt, description, endedAt, isLive, recordingUrl, startedAt, status, thumbnailUrl, title, updatedAt, userId, viewerCount Notifications table: recipientUserId, createdAt, actorAvatarUrl, actorName, actorUserId Comments table: videoId, commentId, content, createdAt, isDeleted, isEdited, likeCount, parentCommentId, replyCount, updatedAt, userAvatarUrl, userId, userName CommentLikes table: commentId, userId, createdAt Channels table: channelId, avatarUrl, channelArn, createdAt, currentStreamId, description, ingestEndpoint, isLive, name, playbackUrl, streamKeyArn, subscriberCount, userId, videoCount Containerization:\nCreate Docker containers for API services Push images to Elastic Container Registry Configure App Runner for automatic deployment Phase 3: Frontend Development React Application:\nImplement responsive UI components Integrate AWS Amplify SDK for authentication Build video upload interface with progress tracking Create video player with adaptive streaming Key Features:\nUser registration/login with email verification Video upload with drag-and-drop functionality Real-time video streaming with quality selection User dashboard for content management Phase 4: Streaming Integration AWS Lambda \u0026amp; MediaConvert Setup:\nCreate Lambda functions for S3 event handling Implement video processing workflow automation Configure MediaConvert job templates for transcoding Set up output presets (1080p, 720p, 480p) Generate HLS/DASH manifests for adaptive streaming Update DynamoDB with processing status Amazon IVS Setup:\nConfigure streaming channels and playback URLs Implement adaptive bitrate streaming Set up recording and archival workflows CloudFront Configuration:\nCreate distributions for video content delivery Configure edge locations for global reach Implement caching strategies for optimal performance Phase 5: CI/CD Pipeline Automated Deployment:\nConfigure CodePipeline for source-to-production workflow Set up CodeBuild for automated testing and building Implement blue-green deployment strategy Testing Strategy:\nUnit tests for API endpoints Integration tests for AWS service interactions Load testing for performance validation 5.Timeline \u0026amp; Milestones Project Duration: 8 Weeks (2 Months) Week 1: Setup \u0026amp; Planning\nAWS account setup and IAM configuration Project requirements finalization Team roles assignment Basic infrastructure deployment (S3, DynamoDB, Cognito) Week 2-3: Backend Development\nRESTful APIs with .NET JWT authentication with Cognito Video upload endpoints Database schemas implementation App Runner deployment Week 4-5: Frontend Development\nReact application with responsive design User authentication flows Video upload interface Basic video player Amplify deployment Week 6: Integration \u0026amp; Streaming\nFrontend-backend integration Lambda functions for video processing automation MediaConvert setup for video transcoding CloudFront setup for video delivery Basic streaming functionality Testing and bug fixes Week 7-8: Final Deployment\nProduction deployment User acceptance testing Documentation completion Project presentation preparation Key Milestones Milestone 1 (Week 1): Infrastructure Ready\nAWS services configured Development environment accessible Milestone 2 (Week 3): Backend Complete\nAPIs functional Authentication working Milestone 3 (Week 5): Frontend Complete\nUI fully developed Basic video upload/playback working Milestone 4 (Week 8): Production Launch\nSystem deployed and tested Documentation complete 6.Budget Estimation Monthly Operating Costs (USD) Compute Services:\nApp Runner (1 services): $5-15/month AWS Lambda: $0-2/month Amplify Hosting: $0-5/month Storage \u0026amp; Database:\nS3 Storage: $0-1/month DynamoDB: $0-2/month CloudFront Data Transfer: $0-2/month Streaming Services:\nAmazon IVS (100 hours/month): $150-300/month AWS Elemental MediaConvert: $20-50/month Security \u0026amp; Monitoring:\nCognito: $0/month CI/CD\nCodePipeline \u0026amp; CodeBuild: $1-3/month\nERC: $0-1/month\nCalculator\nTotal Monthly Cost: $17-44/month\n7.Risks Assessment Primary Risks Technical Risks:\nIntegration complexity → Start simple, gradually increase Time management → Build buffer time, prioritize core features Resource Risks:\nExceeding AWS Free Tier → Monitor usage, set up alerts Mitigation Solutions Technical Management:\nCloudFormation templates Phase-by-phase testing Dev/staging environments Contingency Plans:\nMVP: Basic video upload/playback Core: User auth + streaming Advanced: Live streaming (optional) Use AWS Educate credits Mock services for demos 8.Expected Outcomes Performance Metrics System Performance:\nVideo upload success rate: \u0026gt;95% Streaming latency: \u0026lt;3 seconds System uptime: \u0026gt;99% Concurrent users: 100+ Page load times: \u0026lt;2 seconds Success Criteria MVP Requirements:\nUser registration/login Basic video upload/playback Secure authentication Responsive interface System monitoring Stretch Goals:\nLive streaming capabilities Advanced analytics Social features Mobile companion appld buffer time, prioritize core features Resource Risks:\nExceeding AWS Free Tier → Monitor usage, set up alerts Mitigation Solutions Technical Management:\nCloudFormation templates Phase-by-phase testing Dev/staging environments Contingency Plans:\nMVP: Basic video upload/playback Core: User auth + streaming Advanced: Live streaming (optional) Use AWS Educate credits Mock services for demos 8.Expected Outcomes Performance Metrics System Performance:\nVideo upload success rate: \u0026gt;95% Streaming latency: \u0026lt;3 seconds System uptime: \u0026gt;99% Concurrent users: 100+ Page load times: \u0026lt;2 seconds Success Criteria MVP Requirements:\nUser registration/login Basic video upload/playback Secure authentication Responsive interface System monitoring Stretch Goals:\nLive streaming capabilities Advanced analytics Social features Mobile companion app Attachments / References Document\nVideo\nSlide\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Goals: NAT Gateway Explore AWS Budgets Switch IAM Role Set up Hybrid DNS with Route 53 Resolver Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - NAT Gateway + Create Elastic IP\n+ Deploy NAT Gateway + Configure Route Table for Private Subnet 18/08/2025 18/08/2025 https://000003.awsstudygroup.com/vi/4-createec2server/4.2-connectec2/ 3 - Explore AWS Budgets + Create Budget + Create Cost Budget 19/08/2025 19/08/2025 https://000007.awsstudygroup.com/vi/2-usage-budget 4 - Switch IAM Role + Create IAM Group and IAM User + Create IAM Role and IAM User + Switch IAM Role 20/08/2025 20/08/2025 https://000002.awsstudygroup.com/vi/4-switch-roles 5 - Set up Hybrid DNS with Route 53 Resolver + Set up Hybrid DNS with Route 53 Resolver and test domain name + Create Key Pair + Initialize CloudFormation Template 21/08/2025 21/08/2025 https://000010.awsstudygroup.com/vi/2-prerequiste 6 - Set up Hybrid DNS with Route 53 Resolver\n+ Configure Security Group + Connect RDGW + Install + Create Route 53 Outbound EndpointDNSvolume 22/08/2025 22/08/2025 https://000010.awsstudygroup.com Week 2 Achievements: Mastered and deployed basic AWS services:\nUnderstood and deployed NAT Gateway: Created Elastic IP Created and configured NAT Gateway Updated Route Table for Private Subnet to ensure Internet connectivity for instances. Cost Management:\nGot familiar with AWS Budgets Created basic Budget Created Cost Budget as required to monitor and control expenses. Practiced switching IAM Role:\nCreated IAM Group and IAM User Created IAM Role Successfully switched roles for flexible access management. Set up Hybrid DNS with Route 53 Resolver:\nConfigured Hybrid DNS and tested domain name resolution Created Key Pair \u0026amp; initialized CloudFormation Template Configured Security Group, connected RDGW, set up environment Created Route 53 Outbound Endpoint and verified operation. "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Driving European innovation: Partner solutions for AWS European Sovereign Cloud Catalog of partner solutions for the AWS European Sovereign Cloud: EU data sovereignty focus, categories across Data \u0026amp; Analytics, Observability, Integration/DevOps, AI/ML, and Security/Identity; highlights partners (e.g., ClickHouse, Cloudera, MongoDB, Snowflake, Boomi, Mistral AI, CyberArk, Thales) and the independent EU-operated cloud commitment.\nBlog 2 - Optimizing AWS Elemental MediaLive costs Explains MediaLive pricing and practical cost controls: trimming unused inputs/outputs, right-sizing channel pipelines, scheduling, and monitoring/reporting patterns to reduce total run costs.\nBlog 3 - Automating audio editing \u0026amp; transcoding on AWS Shows how to clip, fade in/out, and transcode audio (FLAC/WAV → AAC) with FFmpeg, Python, EC2, and S3; includes a sample script and a 10-minute cron setup to process new files, setting up for a later serverless/Lambda/MediaConvert evolution.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.3-createrepository/","title":"Create Repository","tags":[],"description":"","content":" Open the Amazon ECR In the navigation pane, choose Repositories, then click Create repository: In the Create repository console: Enter name Repository Click Create After create "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Goals: Set up VPC Peering Learn about AWS Transit Gateway Deploy Wordpress on AWS Cloud Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up VPC Peering + Initialize CloudFormation Template + Create Security Group + Create EC2 instance + Update Network ACL 25/08/2025 25/08/2025 https://000019.awsstudygroup.com/vi/1-introduce/ 3 - Set up VPC Peering + Create Peering Connection + Enable Cross-Peer DNS + Clean up resources 26/08/2025 26/08/2025 https://000019.awsstudygroup.com/vi/6-crosspeerdns/ 4 - Learn about AWS Transit Gateway - Practice: + Create Key Pair + Initialize CloudFormation Template + Create Transit Gateway 27/08/2025 27/08/2025 https://000020.awsstudygroup.com/vi/ 5 - Learn about AWS Transit Gateway Practice: + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables + Clean up resources 28/08/2025 28/08/2025 https://000020.awsstudygroup.com/vi/ 6 - Deploy Wordpress on AWS Cloud Practice: + Prepare VPC and Subnet + Create Security Group for EC2 + Create Security Group for Database Instance + Initialize EC2 Instance + Initialize Database Instance 29/08/2025 29/08/2025 https://000021.awsstudygroup.com/ Week 3 Achievements: Learned Networking – VPC Peering\nInitialized CloudFormation Template for testing environment Created Security Group for EC2 instance Created EC2 instance for Peering connection Updated Network ACL to ensure connectivity between subnets Created VPC Peering connection between two VPCs Enabled Cross-Peer DNS Resolution for domain name resolution via Peering Cleaned up resources after successful testing Learned Networking – AWS Transit Gateway\nUnderstood the role of Transit Gateway in connecting multiple VPCs and on-premise networks Created, attached, and managed Transit Gateway Attachments and Route Tables for centralized traffic routing Ensured stable and scalable routing compared to VPC Peering Practiced initial deployment of Wordpress on AWS Cloud:\nSuccessfully deployed Wordpress environment with EC2 Instance (application) and Database Instance (MySQL/RDS) Set up networking (VPC, Subnet) and security permissions using Security Groups to ensure application safety Connected Wordpress application to the database and successfully tested the deployment "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in three events, each offering distinct learnings and experiences.\nEvent 1 Event Name: AWS AI/ML \u0026amp; Bedrock Workshop\nDate \u0026amp; Time: 8:30 – 12:00 , November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 – 17:00, November 17, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 8:30 – 12:00, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.4-createcodebuild/","title":"Create Code Build","tags":[],"description":"","content":" First, you need add file buildspec.yml in your source code with content like: version: 0.2 env: variables: AWS_DEFAULT_REGION: ap-northeast-1 # change your region ACCOUNT_ID: ############ # change your AWS account ID REPO_NAME: videoshare/repository # change your name repository phases: pre_build: commands: - echo Logging in to Amazon ECR... - aws --version - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com - REPOSITORY_URI=$ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$REPO_NAME - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - echo Repository URI $REPOSITORY_URI - echo Image Tag $IMAGE_TAG build: commands: - echo Building the Docker image for .NET API... - docker build -t $REPO_NAME:$IMAGE_TAG -f backend-video-sharing-platform/Dockerfile . - docker tag $REPO_NAME:$IMAGE_TAG $REPOSITORY_URI:latest post_build: commands: - echo Pushing Docker image to Amazon ECR... - docker push $REPOSITORY_URI:latest - echo Writing imagedefinitions.json for CodePipeline/App Runner... - printf \u0026#39;[{\u0026#34;name\u0026#34;:\u0026#34;%s\u0026#34;,\u0026#34;imageUri\u0026#34;:\u0026#34;%s\u0026#34;}]\u0026#39; \u0026#34;$REPO_NAME\u0026#34; \u0026#34;$REPOSITORY_URI:$IMAGE_TAG\u0026#34; \u0026gt; imagedefinitions.json - echo Build completed successfully! artifacts: files: - imagedefinitions.json Commit github\nOpen the Amazon CodeBuild\nIn the navigation pane, choose Build projects, then click Create project: In the Create project console:\nEnter project name Add Source 1 In field Source provider choose GitHub (If you not have Credential then you need connect it) Choose Repository In Buildspec choose Use a buildspec file Click Create build project "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Goals: Continue deploying Wordpress on AWS Cloud Get familiar with Amazon Relational Database Service (Amazon RDS) Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploy Wordpress on AWS Cloud + Install httpd service + Install php-mysql + Install php7.3 + Move the directory to where Wordpress will be executed to proceed with downloading and installing Wordpress + Open a web browser and access the EC2 webserver\u0026rsquo;s public IPv4 DNS address + Set up basic parameters for Wordpress to log in to the Wordpress admin interface 01/09/2025 01/09/2025 https://000021.awsstudygroup.com/ 3 - Deploy Wordpress on AWS Cloud + Create Autoscaling for Wordpress Instance: + Create AMI from Webserver Instance DNS + Create Launch Template + Create Target Group + Create Load Balancer + Create Auto Scaling Group 02/09/2025 02/09/2025 https://000021.awsstudygroup.com/ 4 - Deploy Wordpress on AWS Cloud + Backup and restore database + Create CloudFront for Web Server + Clean up resources 03/09/2025 03/09/2025 https://000021.awsstudygroup.com/ 5 - Get familiar with Amazon Relational Database Service (Amazon RDS) Practice: + Create VPC + Create EC2 Security Group + Create RDS Security Group + Create DB Subnet Group 04/09/2025 04/09/2025 https://000005.awsstudygroup.com/vi/4-create-rds/ 6 - Get familiar with Amazon Relational Database Service (Amazon RDS) Practice: + Create EC2 instance + Create RDS database instance + Deploy application + Backup and restore + Clean up resources 05/09/2025 05/09/2025 https://000005.awsstudygroup.com/vi/4-create-rds/ Week 4 Achievements: Deployed Wordpress on AWS Cloud\nSuccessfully installed Wordpress on EC2 Webserver, accessible via IPv4 Public DNS. Set up basic parameters and logged in successfully. Built Autoscaling mechanism for Wordpress Instance using AMI, Launch Template, Load Balancer, Target Group, and Auto Scaling Group, enabling flexible system scalability. Performed backup and restore of the Wordpress database to ensure data availability. Integrated CloudFront to distribute static content, increase access speed, and improve user experience. Got familiar with Amazon Relational Database Service (Amazon RDS)\nLearned how to operate RDS and related components: VPC, Security Group, DB Subnet Group. Successfully created RDS Database Instance and connected it to EC2 for application deployment. Practiced management operations: backup, restore, and resource cleanup. "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.5-createcodepipeline/","title":"Create Code PipeLine","tags":[],"description":"","content":" In the navigation pane, choose Pipelines, then click Create repository: In the Create new pipeline console choose Build custom pipeline: Enter name pipeline Choose source GitHub, create connection and choose repository Choose Other build providers Choose AWS CodeBuild Choose project created Choose skip test stage Choose skip deploy stage Create pipeline "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Goals: Get familiar with S3 bucket and create EC2 Instance using AMI Storage Gateway. Deploy FCJ Management application with Auto Scaling Group. Practice with Amazon CloudWatch Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get familiar with S3 bucket and create EC2 Instance using AMI Storage Gateway + Create S3 Bucket + Create EC2 for Storage Gateway + Use AWS Storage Gateway + Clean up resources 08/09/2025 08/09/2025 https://000024.awsstudygroup.com/1-introduce/ 3 - Deploy FCJ Management application with Auto Scaling Group + Set up network infrastructure + Initialize EC2 Instance\n+ Initialize Database Instance with Amazon RDS + Set up data for Database + Deploy web server + Prepare metrics for Predictive scaling 09/09/2025 09/09/2025 https://000006.awsstudygroup.com/vi/6-create-auto-scaling-group/ 4 - Deploy FCJ Management application with Auto Scaling Group Practice: + Set up Load Balancer + Create Target Group + Create Load Balancer 10/09/2025 10/09/2025 https://000006.awsstudygroup.com/vi/6-create-auto-scaling-group/ 5 - Deploy FCJ Management application with Auto Scaling Group + Check results + Create Auto Scaling Group + Test solutions + Clean up resources 11/09/2025 11/09/2025 https://000006.awsstudygroup.com/vi/6-create-auto-scaling-group/ 6 - Practice with Amazon CloudWatch Practice: + Deploy CloudFormation Stack + CloudWatch Metric + CloudWatch Logs + CloudWatch Alarms + CloudWatch Dashboards + Clean up resources 12/09/2025 12/09/2025 https://000008.awsstudygroup.com/vi/5-cloud-watch-alarm/ Week 5 Achievements: Got familiar with Amazon S3 and Storage Gateway\nSuccessfully created and managed S3 Bucket. Initialized EC2 Instance to install and use AWS Storage Gateway. Understood how to connect on-premise storage and cloud via Storage Gateway, supporting efficient data storage. Deployed FCJ Management application with Auto Scaling Group\nFully set up network infrastructure, initialized EC2 Instance and Database Instance (Amazon RDS), deployed data and web application. Successfully configured Load Balancer + Target Group, ensuring load distribution between instances. Created and tested Auto Scaling Group, enabling the system to automatically scale in and out based on actual load. Checked and evaluated scaling solutions, ensuring stable and flexible application operation. Practiced with Amazon CloudWatch\nLearned and deployed CloudWatch Metrics, Logs, Alarms, and Dashboards for system monitoring. Built real-time alerts, supporting effective AWS resource management and optimization. "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/","title":"5. Workshop","tags":[],"description":"","content":"Setup Code PileLine and Code Build for Elastic Container Registry Overview Setup CodePipeline and CodeBuild for Amazon ECR is a streamlined and automated approach to building, testing, and deploying container images into your Elastic Container Registry. This workshop will guide you through the essential steps to create a fully automated CI/CD workflow\nThrough this workshop, you will:\nUnderstand the core concepts of CodePipeline and CodeBuild\nLearn how to configure a secure and automated pipeline.\nExplore how CodeBuild builds and pushes Docker images to ECR\nPractice setting up buildspec files for containerized applications\nAutomate the process of storing and updating images in ECR\nLet’s begin our journey into automating container image delivery with CodePipeline and CodeBuild!\nMain content Introduction Preparation Create Repository Create Code Build Create Code Pipeline Test Result Clean up "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Goals: Use AWS IAM Identity Center for identity management. Get familiar with AWS Web Application Firewall. Manage resources using Tags and Resource Groups. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Use AWS IAM Identity Center for robust identity management + Access AWS CLI + Customer Managed Policies + Manage and audit User and Group activities in AWS IAM Identity Center + Clean up resources 13/10/2025 13/10/2025 https://000012.awsstudygroup.com/ 3 - Learn concepts and practice with AWS Web Application Firewall + Introduction to AWS WAF + Create S3 bucket + Use OWASP Juice Shop to deploy sample application 14/10/2025 14/10/2025 https://000026.awsstudygroup.com/ 4 - Learn concepts and practice with AWS Web Application Firewall - Use AWS WAF Practice: + Deploy Web ACLs with managed rules + Create custom rules to handle requests + Define rules using JSON to manage versions and easily review how, when, and why a set of complex rules was changed + Test new rules + Use Amazon Kinesis Firehose to log requests + Clean up resources 15/10/2025 15/10/2025 https://000026.awsstudygroup.com/vi/ 5 - Manage resources using Tags and Resource Groups - Guide on assigning metadata to each resource as Tags + Tag resources, add/remove tags, manage and filter tagged resources + Use tags on Console: perform operations with tags on AWS Management Console + Create EC2 Instance with tags + Add or remove tags + Filter resources by tag + Use tags via CLI 16/10/2025 16/10/2025 https://000027.awsstudygroup.com/vi/1-using-tags 6 - Manage resources using Tags and Resource Groups Practice: + Use tags via CLI + Add tags to existing resources + Add tags to new resources + Describe tagged resources + Create a Resource Group: create a Resource Group classified by tags + Clean up resources 17/10/2025 17/10/2025 https://000027.awsstudygroup.com/vi/1-using-tags Week 6 Achievements: Identity management with AWS IAM Identity Center\nLearned how to centrally manage users, groups, and access permissions using IAM Identity Center. Applied Customer Managed Policies for detailed access control. Practiced managing, monitoring, and auditing User/Group activities via CLI and Console. Learned and practiced AWS Web Application Firewall\nMastered the concept and role of WAF in protecting web applications from security threats. Deployed sample application using OWASP Juice Shop for testing. Practiced creating Web ACLs with managed and custom (JSON) rules, controlled and logged requests via Amazon Kinesis Firehose. Built a WAF testing environment, understood how to apply and test security rules. Resource management with Tags and Resource Groups:\nUnderstood the tagging mechanism to attach metadata to AWS resources. Tagged, added, and removed tags for resources via Console and CLI, managed and filtered resources effectively. Created Resource Groups based on tags, helping classify and organize resources into groups for easier system administration. "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AWS from 08/09/2025 to 09/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in activities focused on AI/ML, DevOps, and Security on AWS, through which I improved my skills in programming, analysis, reporting, and communication.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.6-testresult/","title":"Test Result","tags":[],"description":"","content":"Once the setup is complete, the automated CI/CD workflow operates as follows:\nCode Changes Pushed A developer commits and pushes new code to the source repository (GitHub or CodeCommit).\nPipeline Triggered CodePipeline automatically detects the change and starts the pipeline.\nSource Stage Executes CodePipeline fetches the latest version of the source code and passes it to the next stage. Build Stage (CodeBuild) CodeBuild pulls the source code, reads the buildspec.yml, logs in to ECR, builds the Docker image, tags it, and pushes it to the correct ECR repository. (you can see process in pipeline) Image Published to ECR The newly built Docker image is stored and versioned in Amazon ECR.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Detailed feedback after the AWS internship (08/09/2025 – 09/12/2025) to help the FCJ/AWS team improve next cohorts:\nOverall Evaluation 1. Working Environment\nFriendly, open; teammates respond quickly even off-hours when needed. Workspace is tidy and quiet enough for labs and writing. Learning environment is systematic with a clear roadmap from week 1 to week 12. 2. Support from Mentor / Team Admin\nMentor explains clearly, encourages questions, lets me try first then reviews for deeper learning. Admin handles access, paperwork, and docs quickly; reminds deadlines and helps schedule labs. 1-on-1 sessions between mentor and interns are very helpful for quick problem-solving. Community channels are active; all questions answered within hours. 3. Relevance to Major\nTasks align with Cloud/DevOps/AI background; added Security (IAM, guardrails) to round out the view. Weekly pacing is reasonable with a clear roadmap, so no overload. Coverage spans from AWS basics (EC2, S3) to advanced services (Lambda, DynamoDB, Cognito, IVS). Real-world project experience building and deploying actual applications from scratch. 4. Learning \u0026amp; Skill Development\nTechnical: covered core cloud foundations, CI/CD, IaC, observability, and GenAI/ML. Soft skills: reporting, demo presentations, balancing lab time and writing. Exposure to cutting-edge technologies and industry best practices. Internship helps build a strong portfolio for future job applications. 5. Culture \u0026amp; Team Spirit\nRespectful, collaborative, outcome- and learning-focused. In urgent cases, everyone helps regardless of title. Positive vibe makes it easy to ask for reviews and share blockers. Regular team meetings to sync progress, share knowledge, and solve problems together. 6. Internship Policies / Benefits\nFlexible hours when needed. Access to internal trainings and hands-on workshops (AI/ML, DevOps, Security). Sufficient AWS credits provided for labs without budget concerns. Networking opportunities with experts and mentors from AWS and leading tech companies. "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Goals: Manage access to EC2 Resource Tag service with AWS IAM. Get started with Grafana on AWS. LIMIT USER PERMISSIONS WITH IAM PERMISSION BOUNDARY Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Manage access to EC2 Resource Tag service with AWS IAM - Create policies with roles that can be used by certain users + Create IAM User + Create IAM Policy Practice: + Create IAM Role + Log in to AWS Management Console and access IAM Management Console. + Create role + Create an IAM role for EC2 Administrator user with the policies created above. 20/10/2025 20/10/2025 https://000028.awsstudygroup.com 3 - Manage access to EC2 Resource Tag service with AWS IAM. - Test policies\n+ Switch Role + Check IAM Policy\n+ Access EC2 console in AWS Region - Tokyo + Access EC2 console in AWS Region - North Virginia + Create EC2 instance with and without Tags that meet the conditions\n+ Edit Resource Tag on EC2 Instance\n+ Test policies + Clean up resources 21/10/2025 21/10/2025 https://000028.awsstudygroup.com 4 - Get started with Grafana on AWS - Preparation steps: Practice: + Create VPC and subnet + Create Security Group + Create EC2 Instance + Create IAM User + Create IAM Role + Assign IAM Role\n- Install Grafana\n- Monitor with Grafana 22/10/2025 22/10/2025 https://000029.awsstudygroup.com 5 - LIMIT USER PERMISSIONS WITH IAM PERMISSION BOUNDARY - Prepare AWS account with IAM enabled. + Create Policy + Copy the JSON from the workshop into the field + Skip tagging and select Next: Review\n+ Name the policy ec2-admin-restrict-region. 23/10/2025 23/10/2025 https://000030.awsstudygroup.com/vi/3-createpolicy 6 - LIMIT USER PERMISSIONS WITH IAM PERMISSION BOUNDARY Practice: + Create an IAM user and apply permission boundary to the user + Check Limited IAM User + Check if the user assigned AmazonEC2FullAccess is restricted by Permission Boundary + Clean up resources 24/10/2025 24/10/2025 https://000030.awsstudygroup.com/vi/3-createpolicy Week 7 Achievements: Managed access to EC2 service with AWS IAM (Resource Tag):\nUnderstood and applied IAM Policy mechanism to control EC2 access based on Resource Tag. Created and assigned IAM Role/Policy to users, thereby restricting EC2 instance operations according to defined tags. Successfully tested policies by creating/editing EC2 instances in different regions. Got started with Grafana on AWS:\nInstalled and deployed Grafana on EC2. Understood IAM Role assignment and data monitoring integration. Built a basic monitoring environment with Grafana, enabling more visual monitoring of AWS systems and applications. Limited permissions with IAM Permission Boundary:\nLearned the concept of Permission Boundary to control the maximum scope of IAM User permissions. Deployed permission boundary policies. Successfully tested: even when the user is granted AmazonEC2FullAccess, actions outside the allowed scope are still restricted. "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/5-workshop/5.7-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations! You’ve successfully completed the workshop and built your own automated CI/CD pipeline with CodePipeline, CodeBuild, and Amazon ECR. Great job!\nClean up First you need delete S3 bucket of code Pipeline Choose Code Pipeline you want delete and click Delete Pipeline Choose Code Build you want delete and click Action, choose Delete Choose Repository and click delete "},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Goals: Prepare and set up resources for the workshop. Optimize EC2 costs with Lambda. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Prepare and set up resources for the workshop. + Download Hugo Theme\n+ Download Snagit + Download Active Presenter + Download Draw.io 27/10/2025 27/10/2025 3 - Optimize EC2 costs with Lambda + Create VPC + Create Security Group\n+ Create EC2 instance + Incoming Web-hooks slack + Tag Instance 28/10/2025 28/10/2025 https://000022.awsstudygroup.com/ 4 - Optimize EC2 costs with Lambda + Create Role for Lambda + Create Lambda Function + Function to stop instance + Function to start instance\n+ Check results\n+ Clean up resources 29/10/2025 29/10/2025 https://000022.awsstudygroup.com 5 30/10/2025 30/10/2025 6 31/10/2025 31/10/2025 Week 8 Achievements: Successfully prepared all necessary resources and tools for workshop creation (Hugo Theme, Snagit, Active Presenter, Draw.io).\nMastered EC2 cost optimization techniques using Lambda functions.\nImplemented automated EC2 instance start/stop scheduling with Lambda.\nIntegrated Slack notifications for instance management alerts.\nLearned proper EC2 instance tagging and resource organization.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Grant AWS service access to applications using IAM Role. Understand database systems and AWS Database services. Deploy Amazon RDS on AWS. Build a Data Lake on AWS. Amazon DynamoDB. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resource 2 - Grant application permissions to access AWS services with IAM Role: + Create EC2 Instance + Create S3 bucket + Use access key + Create IAM user and access key + Use access key + IAM role on EC2 + Create IAM role + Use IAM role + Clean up 2/11/2025 2/11/2025 https://000048.awsstudygroup.com/vi/ 3 - Learning: + Database Concepts + Amazon RDS + Amazon Aurora + Amazon Elasticache + Amazon Redshift 3/11/2025 3/11/2025 4 - Deploy Amazon RDS: + Create VPC + Create EC2 Security Group/Create EC2 instance + Create RDS Security Group/Create RDS database instance + Create DB Subnet Group + Deploy application + Backup and restore + Clean up 4/11/2025 4/11/2025 https://000005.awsstudygroup.com/vi/ 5 - DataLake on AWS: + Create IAM Role + Create Policy + Collect and store data + Create S3 bucket + Create Delivery Stream + Create sample data + Create Data Catalog: + Create Glue Crawler + Verify data + Transform data + Analyze and visualize + Analyze with Athena + Visualize with QuickSight + Clean up resources 5/11/2025 5/11/2025 https://000035.awsstudygroup.com/vi/1-introduce/ 6 - Amazon DynamoDB: + Explore DynamoDB + Explore DynamoDB console + Backup and restore + Clean up 6/11/2025 6/11/2025 https://000039.awsstudygroup.com/vi/ Week 9 Achievements: IAM Role grants application access permissions\nMastered knowledge about Database on AWS.\nSuccessfully deployed Amazon RDS.\nBuilt a complete Data Lake on AWS.\nCompleted DynamoDB learning and practice.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: AWS Glue \u0026amp; Amazon Athena. Work with Amazon DynamoDB. Analytics on AWS Workshop. Migrate Monolith application to AWS environment. Serverless – Lambda interaction with S3 and DynamoDB. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resource 2 - AWS Glue \u0026amp; Amazon Athena: + Configure AWS Glue and Crawler + Create S3 bucket to store query results + Analyze cost and performance usage (Amazon Athena) + Clean up 7/11/2025 7/11/2025 https://000040.awsstudygroup.com/vi/ 3 - Work with Amazon DynamoDB: + CloudShell 2.2 + Console + Analyze cost and performance usage (Amazon Athena) + SDK 8/11/2025 8/11/2025 https://000060.awsstudygroup.com/vi/ 4 - Analytics on AWS workshop: + Design serverless Data Lake architecture + Build data processing pipeline and Data Lake using Amazon S3 + Use Amazon Kinesis + Use Amazon Kinesis Data Analytics + \u0026hellip; + Clean up 10/11/2025 10/11/2025 https://000072.awsstudygroup.com/vi/ 5 - Migrate Monolith Application: + Preparation + Test on local server using Eclipse IDE + Deploy on ElasticBeanstalk + Update application + Query API using Eclipse IDE + Clean up 11/11/2025 11/11/2025 https://000050.awsstudygroup.com/vi/ 6 - Serverless - Lambda interaction with S3 and DynamoDB: + Process and optimize image size on AWS + Write data to Amazon DynamoDB + Clean up 12/11/2025 12/11/2025 https://000078.awsstudygroup.com/vi/ Week 10 Achievements: AWS Glue \u0026amp; Amazon Athena\nSuccessfully configured Glue Crawler and created Data Catalog. Created S3 bucket and set up query result storage for Athena. Performed queries, evaluated performance and analyzed Athena costs. Work with Amazon DynamoDB.\nSuccessfully performed DynamoDB operations via CloudShell and Console. Clearly analyzed DynamoDB cost model and read/write performance. Practiced DynamoDB via SDK and tested various APIs. Analytics on AWS Workshop.\nDesigned complete serverless Data Lake architecture. Built data pipeline using Amazon S3 and Amazon Kinesis. Set up Kinesis Data Analytics for real-time data processing and analysis. \u0026hellip; Migrate Monolith Application\nTested application in local environment using Eclipse IDE. Successfully deployed application to Elastic Beanstalk. Updated application and tested API through Eclipse IDE. Successfully built Lambda to process and optimize image size when uploading to S3, saved processing result metadata to DynamoDB.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Write Frontend to call API Gateway. Deploy applications using AWS SAM. Authentication with Amazon Cognito. Set up static website with SSL on S3. CI/CD with CodePipeline. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resource 2 - Serverless - Guide to writing Frontend to call API Gateway: + Deploy front-end + Deploy Lambda function + Configure API Gateway + Test API with Postman + Test API with front-end + Clean up 13/11/2025 13/11/2025 https://000079.awsstudygroup.com/vi/ 3 - Lab-Serverless - Deploy application using SAM: + Deploy front-end + Deploy Lambda function + Configure API Gateway + Test API with Postman + Test API with front-end + Clean up 14/11/2025 14/11/2025 https://000080.awsstudygroup.com/vi/ 4 - Lab-Serverless - Authentication with Amazon Cognito: + Preparation + Create User Pool + Create API and Lambda function + Test with front-end + Clean up 15/11/2025 15/11/2025 https://000081.awsstudygroup.com/vi/ 5 - Lab-Serverless - Set up static website with SSL on S3: + Preparation + Create queue and SNS topic + Create API and Lambda function + Test operations + Clean up 17/11/2025 17/11/2025 https://000082.awsstudygroup.com/vi/ 6 - Serverless - Handle orders with SQS-SNS: + Preparation + Create domain and Hosted zone\n+ Request SSL certificate + Create CloudFront distribution + Clean up 18/11/2025 18/11/2025 https://000083.awsstudygroup.com/vi/ Week 11 Achievements: Serverless – Frontend calling API Gateway\nSuccessfully deployed frontend and connected with backend. Lambda function operates with correct logic. API Gateway configured correctly (method, integration, CORS). Successfully tested API via Postman and frontend interface. Deploy application using AWS SAM\nAuthentication with Amazon Cognito\nSet up static website with SSL on S3\nHandle orders with SQS-SNS\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Serverless CI/CD with CodePipeline. Workshop. Tasks to be implemented this week: Day Tasks Start Date Completion Date Resource 2 - Lab - Serverless - CI/CD with CodePipeline: + Preparation + Build SAM pipeline + Build frontend pipeline + Test operations + Clean up 18/11/2025 18/11/2025 https://000084.awsstudygroup.com/vi/ 3 - Practice: + Complete WORKSHOP 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: + Complete WORKSHOP 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Complete WORKSHOP 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Complete WORKSHOP 22/11/2025 22/11/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Mastered the process of building serverless systems on AWS.\nProficient in setting up and operating CodePipeline.\nKnow how to configure, deploy and test the entire CI/CD pipeline.\nCompleted all workshop requirements fully.\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/3-blogstranslated/3.1-blog1/aws-partner-network-apn-blog/","title":"","tags":[],"description":"","content":"AWS Partner Network (APN) Blog\nThúc đẩy đổi mới sáng tạo châu Âu: Giải pháp đối tác cho AWS European Sovereign Cloud by Max Peterson on 02 JUL 2025\nTrong chuyên mục: APN Launches, Partner Solutions\nTác giả: Max Peterson, Phó Chủ tịch – AWS Sovereign Cloud Khi chúng ta tiến gần hơn đến thời điểm ra mắt AWS European Sovereign Cloud – một đám mây độc lập mới dành cho châu Âu, chúng tôi rất vui mừng khi thấy hệ sinh thái đối tác của mình tiếp tục phát triển mạnh mẽ. AWS European Sovereign Cloud sẽ được đặt và vận hành hoàn toàn trong Liên minh Châu Âu (EU), cung cấp cho khách hàng mức độ bảo mật, khả dụng và hiệu năng tương đương với AWS hiện tại, cùng với các tính năng bổ sung phù hợp với các quy định và yêu cầu chủ quyền dữ liệu của EU đang thay đổi.\nHôm nay, chúng tôi vui mừng giới thiệu danh mục giải pháp đối tác mở rộng đang được phát triển cho AWS European Sovereign Cloud. Các giải pháp mới này bổ sung cho bộ giải pháp mạnh mẽ đã được công bố vào tháng 11 năm ngoái từ các tên tuổi hàng đầu như Adobe, Cisco, Dedalus, Esri, Genesys, GitLab, Mendix, Pega, Trend Micro, và Wiz. Tất cả đang cùng nhau tạo nên một hệ sinh thái toàn diện, cho phép các tổ chức châu Âu đổi mới một cách tự tin.\nSự phát triển đa dạng của các giải pháp đối tác thể hiện động lực mạnh mẽ tại châu Âu và cam kết hỗ trợ nhu cầu chủ quyền dữ liệu đang thay đổi của khu vực. Từ phân tích dữ liệu và bảo mật đến các ứng dụng đặc thù ngành, các đối tác APN của chúng tôi đang nỗ lực để đảm bảo khách hàng có thể tiếp cận nhiều công cụ và dịch vụ được thiết kế nhằm tuân thủ các yêu cầu chủ quyền dữ liệu trong EU. Các giải pháp này sẽ kế thừa những tiêu chuẩn bảo mật nghiêm ngặt và tính độc lập vận hành từ hạ tầng AWS European Sovereign Cloud, đồng thời bổ sung thêm lớp tính năng tập trung vào chủ quyền dữ liệu.\nCác danh mục giải pháp đang mở rộng và những đổi mới nổi bật Data \u0026amp; Analytics Platforms Các giải pháp này giúp tổ chức khai thác dữ liệu mà vẫn duy trì kiểm soát hoàn toàn chủ quyền dữ liệu, đồng thời cung cấp khả năng phân tích mạnh mẽ với các cơ chế quản trị dữ liệu chuyên biệt cho EU. Đối tác đang phát triển các nền tảng giúp khách hàng xử lý, lưu trữ và phân tích dữ liệu trong khi vẫn tuân thủ các quy định của EU.\nClickHouse đang hợp tác để triển khai nền tảng phân tích thời gian thực trên AWS European Sovereign Cloud. Nền tảng này có thể xử lý hàng nghìn tỷ bản ghi mỗi tháng với độ trễ mili giây, đồng thời duy trì yêu cầu nghiêm ngặt về chủ quyền dữ liệu. Giải pháp phân tích hiệu năng cao này sẽ cho phép doanh nghiệp tận dụng phân tích nâng cao và khả năng Generative AI (GenAI) trong khi vẫn đảm bảo dữ liệu được lưu trữ và xử lý trong phạm vi pháp lý châu Âu, tuân thủ GDPR và các quy định bảo mật dữ liệu địa phương.\nCloudera đang phát triển một nền tảng dữ liệu chủ quyền cho AWS European Sovereign Cloud, cho phép khách hàng giữ dữ liệu và metadata của họ hoàn toàn trong biên giới EU. Giải pháp sovereign-by-design này tuân thủ nghiêm ngặt các yêu cầu chủ quyền và quy định địa phương, đặc biệt phù hợp cho các ngành có quy định nghiêm ngặt và tổ chức khu vực công muốn đảm bảo tính toàn vẹn và tuân thủ dữ liệu trong khu vực châu Âu.\nMongoDB chuẩn bị cung cấp nền tảng cơ sở dữ liệu tài liệu hiện đại của mình trên AWS European Sovereign Cloud. Nền tảng cơ sở dữ liệu hợp nhất này giúp đơn giản hóa quá trình phát triển ứng dụng hướng dữ liệu, cung cấp ngôn ngữ truy vấn thân thiện với nhà phát triển và mô hình dữ liệu linh hoạt, được thiết kế cho khả năng mở rộng và tuân thủ chủ quyền dữ liệu.\nReltio đang mở rộng nền tảng hợp nhất dữ liệu SaaS gốc đám mây, Reltio Data Cloud™, sang AWS European Sovereign Cloud. Nền tảng agentic data fabric này cung cấp dữ liệu theo thời gian thực, đáng tin cậy, giàu ngữ cảnh, hỗ trợ vận hành liền mạch, phân tích tức thời và quy trình tự động thông minh, đồng thời đảm bảo tuân thủ chủ quyền dữ liệu. Với Reltio Intelligent Data Graph, nền tảng sẽ cung cấp cái nhìn đa miền toàn diện về khách hàng, sản phẩm, nhà cung cấp… chỉ trong vài mili giây, giúp doanh nghiệp tăng hiệu suất, thúc đẩy tăng trưởng và mang lại trải nghiệm vượt trội trong môi trường được kiểm soát và bảo mật cao.\nSnowflake có kế hoạch mang nền tảng AI \u0026amp; Data Cloud của mình đến AWS European Sovereign Cloud, giúp doanh nghiệp đáp ứng các yêu cầu về chủ quyền và quy định dữ liệu ngày càng thay đổi ở châu Âu. Nền tảng này cung cấp tập hợp các tính năng quản trị dữ liệu và bảo mật toàn diện, hỗ trợ xử lý dữ liệu nhạy cảm và metadata của khách hàng trong môi trường tuân thủ nghiêm ngặt.\nObservability \u0026amp; Process Management Các giải pháp này cung cấp khả năng quan sát sâu vào ứng dụng, hạ tầng và quy trình kinh doanh, đồng thời đảm bảo tất cả dữ liệu và metadata do khách hàng tạo ra đều nằm trong phạm vi EU. Các đối tác đang xây dựng công cụ kết hợp tính năng giám sát mạnh mẽ với khả năng tối ưu hóa quy trình, tuân thủ kiểm soát lưu trú dữ liệu.\nARIS đang phát triển bộ công cụ Process Intelligence toàn diện cho AWS European Sovereign Cloud. ARIS Suite sẽ giúp các tập đoàn lớn xác định, phân tích, mô phỏng, tối ưu và kiểm soát quy trình kinh doanh trong phạm vi EU. Bộ công cụ bao gồm process mining và mô hình hóa quy trình, hỗ trợ quản lý chi phí, kiểm soát quy trình và tuân thủ quy định.\nd.velop chuẩn bị triển khai Hệ thống Quản lý Tài liệu (DMS) mạnh mẽ của mình trên AWS European Sovereign Cloud. Giải pháp này giúp quản lý và lưu trữ tài liệu tuân thủ bảo vệ dữ liệu, tối ưu quy trình nghiệp vụ, nâng cao khả năng cộng tác và đảm bảo tuân thủ các tiêu chuẩn pháp lý khắt khe trong môi trường chủ quyền dữ liệu.\nSumo Logic đang phát triển nền tảng phân tích log gốc đám mây hợp nhất, hỗ trợ AI, trên AWS European Sovereign Cloud. Nền tảng này cung cấp một nguồn dữ liệu duy nhất cho các nhóm Phát triển, Bảo mật và Vận hành (DevSecOps), giúp khách hàng đảm bảo độ tin cậy ứng dụng, bảo vệ trước các mối đe dọa hiện đại và có được cái nhìn sâu sắc về hạ tầng đám mây, đồng thời duy trì chủ quyền dữ liệu.\nIntegration, DevOps \u0026amp; Migration Những công cụ này hỗ trợ phát triển, triển khai, tích hợp và di chuyển phần mềm an toàn, tuân thủ, trong khi vẫn duy trì quyền tự chủ vận hành trong phạm vi EU. Chúng phục vụ toàn bộ chuỗi hiện đại hóa, từ vòng đời phát triển phần mềm, tích hợp dữ liệu, đến vận hành và di chuyển lên đám mây, tất cả đều được thiết kế với chủ quyền dữ liệu châu Âu làm trọng tâm.\nBoomi đang phát triển phiên bản mới của Boomi Enterprise Platform dành riêng cho AWS European Sovereign Cloud. Nền tảng tích hợp và tự động hóa dựa trên AI này kết nối ứng dụng, API, dữ liệu và AI agents, thúc đẩy chuyển đổi số và AI tin cậy trong phạm vi chủ quyền dữ liệu. Boomi đầu tư vào phiên bản riêng này để củng cố niềm tin về bảo mật metadata, tăng cường chủ quyền dữ liệu cho khách hàng tại EU, đồng thời đáp ứng nhu cầu tích hợp ngày càng tăng.\nEficode đang phát triển Eficode ROOT cho AWS European Sovereign Cloud, kết hợp nền tảng phát triển được quản lý với chuyên gia DevOps. Giải pháp này giúp khách hàng xây dựng và vận hành môi trường phát triển bảo mật, mở rộng và tuân thủ trong hạ tầng đám mây chủ quyền.\nTrianz đang phát triển Concierto, một nền tảng SaaS tự động hóa cao, không cần mã, cho AWS European Sovereign Cloud. Nền tảng này tăng tốc quá trình di chuyển và hiện đại hóa đám mây đầu-cuối thông qua tự động hóa thông minh, cung cấp khả năng vận hành đám mây lai, FinOps, tối ưu hóa và khắc phục tự động, tất cả trong giao diện thống nhất (single-pane-of-glass). Giải pháp giúp tổ chức giảm nợ kỹ thuật, duy trì toàn quyền kiểm soát dữ liệu và hạ tầng, đồng thời đạt được tự chủ vận hành trong khi tối ưu hành trình lên đám mây.\nAI \u0026amp; Machine Learning Các giải pháp AI và Machine Learning xử lý và phân tích dữ liệu trong phạm vi EU, cho phép tổ chức châu Âu đổi mới trong khi vẫn đảm bảo chủ quyền dữ liệu.\nMistral AI chuẩn bị cung cấp các mô hình và ứng dụng AI, bao gồm Le Chat Enterprise, trên AWS European Sovereign Cloud. Nền tảng này cung cấp khả năng tìm kiếm doanh nghiệp an toàn, hỗ trợ tạo AI agents tùy chỉnh để tự động hóa nhiệm vụ, cho phép truy cập dữ liệu từ nhiều nguồn mà vẫn tuân thủ quy định chủ quyền dữ liệu. Đặc biệt phù hợp cho các tổ chức yêu cầu triển khai an toàn cao, như quốc phòng, khu vực công và dịch vụ tài chính.\nSecurity \u0026amp; Identity Management Các giải pháp bảo mật và quản lý danh tính giúp bảo vệ khối lượng công việc nhạy cảm trong khi duy trì kiểm soát dữ liệu nghiêm ngặt. Những công cụ này được thiết kế riêng cho môi trường đám mây châu Âu, nhằm tăng cường bảo vệ và tuân thủ.\nCyberArk chuẩn bị cung cấp nền tảng AI-powered Identity Security Platform trên AWS European Sovereign Cloud. Nền tảng này áp dụng kiểm soát đặc quyền thông minh cho mọi danh tính – bao gồm con người, máy móc hoặc AI, giúp giảm thiểu rủi ro vận hành và bảo mật bằng cách triển khai mô hình Zero Trust và Least Privilege, mang lại tầm nhìn đầy đủ trong môi trường chủ quyền dữ liệu.\nThales đang phát triển các giải pháp an ninh mạng tùy chỉnh cho AWS European Sovereign Cloud, giải quyết các thách thức trọng yếu về chủ quyền dữ liệu. Bao gồm CipherTrust Cloud Key Manager để kiểm soát khóa mã hóa, CipherTrust Data Security Platform để bảo vệ dữ liệu toàn diện, Thales CIAM để xác thực người dùng an toàn, và Imperva Web Application and API Protection (WAAP) để duy trì chủ quyền dữ liệu ở tầng ứng dụng. Những giải pháp này giúp doanh nghiệp duy trì quyền kiểm soát, bảo vệ dữ liệu nhạy cảm và tuân thủ quy định, đồng thời tận dụng dịch vụ đám mây một cách an toàn.\nWithSecure đang phát triển giải pháp an ninh mạng gốc đám mây toàn diện và chủ động cho AWS European Sovereign Cloud. Giải pháp này mang lại khả năng quan sát và bảo vệ nâng cao cho khối lượng công việc tại châu Âu, đảm bảo tuân thủ các yêu cầu chủ quyền dữ liệu.\nCác đối tác đang điều chỉnh giải pháp của mình để phù hợp với các yêu cầu chủ quyền kỹ thuật số cốt lõi bao gồm lưu trú dữ liệu (data residency), hạn chế truy cập vận hành, khả năng phục hồi (resiliency) và khả năng sống sót (survivability) – đảm bảo khách hàng có thể xây dựng môi trường toàn diện, tuân thủ và phù hợp với nhu cầu riêng biệt.Khi tiến gần hơn đến ngày ra mắt, chúng tôi sẽ tiếp tục mở rộng hợp tác đối tác, mang đến cho khách hàng nhiều lựa chọn và khả năng hơn nữa. Khu vực đầu tiên của AWS European Sovereign Cloud tại Brandenburg, Đức, được hỗ trợ bởi khoản đầu tư €7,8 tỷ, thể hiện cam kết của chúng tôi đối với chủ quyền kỹ thuật số châu Âu và thành công của các đối tác.\nKhách hàng có thể bắt đầu lên kế hoạch chuyển đổi sang AWS European Sovereign Cloud ngay hôm nay bằng cách hợp tác với các đối tác và kiến trúc sư giải pháp của chúng tôi để thiết kế kiến trúc đám mây chủ quyền phù hợp.\nCùng với các đối tác, chúng tôi đang xây dựng tương lai nơi các tổ chức châu Âu có thể đổi mới một cách tự tin, biết rằng mọi yêu cầu về chủ quyền kỹ thuật số của họ đều được đáp ứng mà không phải thỏa hiệp.\nTAGS: ARIS, AWS European Sovereign Cloud, Boomi, ClickHouse, Cloudera, CyberArk, d.velop, Eficode, Mistral AI, MongoDB, Reltio, Snowflake, Sumo Logic, Thales, Trianz, WithSecure\n"},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://truong14042004.github.io/tranxuantruongFCJ/tags/","title":"Tags","tags":[],"description":"","content":""}]